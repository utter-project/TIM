

## Meeting information

### Meeting date:

January 16th 2023

### Meeting agenda:

1. Marcely would present the statistics for the surveyed data from MS2.
2. closing remarks


### Meeting transcript:

The following is the transcript of a meeting with multiple participants, where each line has a timestamp (e.g. 11:58:37 AM means 11h58mn37s am), the speaker's name and their utterance.

12:01:38 PM Wilker Aziz: We have this.
12:01:41 PM Marcely Zanon Boito: Hello!
12:01:58 PM Vlad Niculae: Hello!
12:02:02 PM Maryam Hashemi Shabestari: No everyone.
12:02:05 PM André Martins: Hello!
12:02:22 PM Maryam Hashemi Shabestari (chat): Hi Marcely Hi Marcely
12:02:24 PM Wilker Aziz: And just open the agenda here on the side cause I, my computer, is being used for zoom. I like it
12:02:26 PM Maryam Hashemi Shabestari (chat): I made you co-host I made you co-host
12:02:26 PM Marcely Zanon Boito (chat): @Maryam could you please record the meeting then? With standard zoom settings. I will take care of teh transcriptions @Maryam could you please record the meeting then? With standard zoom settings. I will take care of teh transcriptions
12:02:50 PM Maryam Hashemi Shabestari: Okay, I I will record the meeting. And I just gave you more silly. The co-host. I don't know if that will work. Okay. Great.
12:02:58 PM Marcely Zanon Boito: Yes, thank you.
12:03:01 PM Maryam Hashemi Shabestari: I will try to see if it is working. Yes, so if everyone's fine, I will record in parallel with Marcel Okay, I may just
12:03:27 PM Marcely Zanon Boito: The recording stuff? Is it normal
12:03:29 PM Maryam Hashemi Shabestari: Yeah, I just see people are still joining again. It's not started. So I I I wanted to make sure it is working. Yeah.
12:03:34 PM Marcely Zanon Boito: Okay.
12:04:02 PM Chryssa Zerva: Hello!
12:04:03 PM Maryam Hashemi Shabestari: Okay.
12:04:12 PM Evgenia Ilia (she/her): Should we get started? Let me just check
12:04:14 PM Maryam Hashemi Shabestari: Yes, sounds good. I will start with the recording
12:04:18 PM Evgenia Ilia (she/her): Okay. Did you see it? Happy New Year. I missed last meeting, so I I was. I didn't get a chance to say that Hey, guys, we're we're starting today with Marcelie presenting on the milestone for work package, too. Right, and we were considering having a chat about the implications of Chat Gpt. But then I guess we might postpone that, because I think Lexi cannot attend today, and then just clearly interested in this, you actually got us to think about the fact that we should be discussing these implications.
12:04:58 PM Evgenia Ilia (she/her): My study. Perhaps you can go on
12:04:59 PM Marcely Zanon Boito: Oh, yes, sure!
12:05:01 PM Evgenia Ilia (she/her): Thank you.
12:05:08 PM Marcely Zanon Boito: Let me just share the Was not supposed to be muted. Are you releasing to me? Are you a bit to listen to me right now? Maybe it was when I shared something out.
12:05:32 PM André Martins: Yes, yes.
12:05:36 PM Marcely Zanon Boito: Happen I'm gonna share the whole window this time. Okay. You're still this hearing me? Right? You're Sioux.
12:05:43 PM André Martins: Yes.
12:05:46 PM Marcely Zanon Boito: Okay.
12:05:47 PM Chryssa Zerva: Yes.
12:05:52 PM Marcely Zanon Boito: Okay, just let's check that. You're able to listen to me because I cannot see the Zoom Meeting anymore.
12:06:00 PM André Martins: Yes.
12:06:00 PM Chryssa Zerva: We can.
12:06:01 PM Marcely Zanon Boito: Okay, that's so. I prepared a quick summarization of what was done for milestone, too.
12:06:09 PM Marcely Zanon Boito: Milestone. Chu was reached at the end of the December, and it was a survey on available data that will be used for all work packages in Utah. So the goal was pretty much it serving all existing relevant data.
12:06:29 PM Marcely Zanon Boito: So it is point we are not 100 adding or making data sets ourselves. We're just checking to see what we already have available, and and we are, of course, interested on data, relevant data. And for us, relevant data is stated that matches or use cases so mainly confessational dialogue data that can be used for custom care and meaning assistant relevant data is data in relevant languages. We have 6 languages that utter Dutch, English, French, German, Portuguese, the European version, and Korean, and relevant data is data in relevant tasks and the tasks that we are listed by interaction with the other work packages and that we discussed during the kickoff
12:07:22 PM Marcely Zanon Boito: meeting, were these tasks that are listed here we have emotion, recognition. Twice from text and from speech. We have machine translation, mining. We have also have supervised learning for speech, model, pre training, speech, translation, speech, recognition and summarization. And then the second go of the milestone by by finding data of this kind is not only knowing that it exists, but identifying the existing data needs.
12:07:56 PM Marcely Zanon Boito: We what we wanted to to accomplish fifties. Milestone was being able to understand. If these tasks are, we are able to do them with the data that we have today available.
12:08:02 PM André Martins (chat): I think Brazilian Portuguese is also relevant, right? (At least textual data) I think Brazilian Portuguese is also relevant, right? (At least textual data)
12:08:11 PM Marcely Zanon Boito: So for the organization, we decided to organize this survey on a collection of tables. Every data set that we collected, we added entries at the table that we call the general table. In this table we have pretty standard information, such as the data set name. The language is, it covers modality, speech, text, or both? Which task it is. It can be used, for from the tests that I listed on the last slide, then we have domain over is it from Wikipedia or are these Ted Talks? This kind of information we also added a size estimation in gigabytes. We added license links for the paper when it has a paper. Download URL and the other use case. It is useful for, and we also had a free field for adding a short description.
12:09:15 PM Marcely Zanon Boito: If we found it was necessary, and then based on the modality of the data, set. If it's speech or text, we had a different table, that we would add information to. So, for instance, if we we had a data set at the general table for whose modality is speech, we also add more detailed information about this data set at the speech that I said stable, and there we will add duration number of few terms is a speech style number of speakers. General information, accent, information, emotion, labeling information about the text. If there is text, aligned to the speech and hugging face data sets link when available, then the same thing for text. We we had our source and target statistics for the text sentence types and tokens emotion, labeling.
12:10:09 PM Marcely Zanon Boito: And how do you face data sets link? So this is what we deliver. A set of tables that can be modified and updated, and we hope they will be useful for the other work. Packages so, and they they are accessible with this link that it's on the screen. I can also put it on the chat later, the organization, the for making these tables was pretty much by modality never labs in Edinburgh. We focused on the speech table and therefore on the tasks that involved a speech and on debo and it they focused on the the textual data sets. So what I wanted to show you. It's a bit of an overview of the data we collected and what we can do with it. So on the general table we collected 91 distinct data sets.
12:11:08 PM Marcely Zanon Boito: That's a roughly 10 TB of data. These results because some data sets have more than one language just results in 207 data set language entries. The. So here you can see the language representation. We see that the French, English, German, they are the top 3 with more entries. Korean is the one of the list data sets, and we have Portuguese and and that's kind of in the middle regarding use cases. Feature, 7% of the data collected was for with sexual data for the customer care assistant. Here we have a meeting assistant, and both, and in this case both means speech. That are from the meeting assistant case. That can also be used for the customer care as assistance.
12:12:08 PM Marcely Zanon Boito: For instance, the transcriptions on ASR. They can be used for customer care, and then in this case it was flagged, as both so pretty much. We have 57 to 2 43 in terms of use case, which is pretty good regarding task representation. We collected 123 machine translation that I said, so this is a task that is very well served, followed by speech. Recognition, speech, translation and summarization. We did find some mining data sets which is nice. So we'll talk about this a bit later, and we found some emotion recognition data sets. But these were a bit difficult to find, due to complicated licenses. For the domains. This is the top 5, the the domains that we had.
12:13:02 PM Marcely Zanon Boito: We have 53 different domains on the general table. The one most represented is legal. Then you have multiple mixed it. Wikipedia and European Parliament sessions. As you can imagine, but by the use case by chart, the modality that is most represented is text, and then we have the speech and text and speech following, focusing on, then on the speech tasks and the speech table, we collected 212,000 h of speech some of this speech was
12:13:40 PM Marcely Zanon Boito: transcribed, and in this case we have over 159 million utterances. This is a lot of data. So we are very happy about it. And this corresponds to 84 language direction pairs. We also have a lot of speech, information, sorry speech with gender information, and we also were able to collect over 14,000 h of accented speech. No languages but Korean, so this should allow us to to do, to build robust ASR models other than that we have for 4 sorry, 460 h of emotional speech, labeled in French English and Korean and 16 speech Styles identified the top 3 being podcasts is
12:14:31 PM Marcely Zanon Boito: Spontaneous and red, so the distribution per language we have over half of this speech that we collected is in English, which is not surprising. Followed, this is followed by Portuguese, German, French. That's in Korean, which is our low resource language in this scenario. If we look by task, we, the thing is very surprising that it's very surprising that we have mini as top one. We found that are set with a very large amount of hours for mining from spotify, and this but minuting is very well covered desk. This is followed by speech, recognition, translation, most recognition. Ssl. And summarization. So now let me so show you what we can do. If this data for speech, so regarding his speech recognition, this is the the overview of the amount of hours for language.
12:15:38 PM Marcely Zanon Boito: What we can see is that for English, German, French, and Dutch, we have over 1,000 h of speech. So these are languages that we are sure that we can easily train and deploy ASR models for Portuguese and Korean, though we do not have a lot of data. This does not mean that it is not possible to perform speech recognition, but we have to leverage extra information, Monomodo information, pre-trained models in order to to accomplish that. Then, if we talk about speech translation, we like a lot of language directions. We find that we are able to mainly perform English to other European languages, for that we have some data sets that allow us to that give us enough data. But for all other cases we like data.
12:16:33 PM Marcely Zanon Boito: So this is something that we we might want to discuss for the meaning assistant use case. What would be the direction of the translations that we would like to cover. So for the moment, what can we do about it? There are 2 options. One would be, get some more data. But how is is a question because we I don't know how we would find more data in. Some of this is specific language pairs, and another option is the focus on cascaded and joint approaches. So joint here means, performing ASR and translation at the same time, which could, could, May made a bit easier to translate with less data, and cascaded. That would simply mean ASR plus the machine translation data that we have.
12:17:24 PM Marcely Zanon Boito: And then here the some observation is, that the if you do so, we should also think about how to deploy this on online settings, because for the meaning assistant use case, we are interested on on online transcription and translation. So it's so. If the model is to complicated or slow, then to be a problem
12:17:51 PM André Martins: Can I ask a question? Actually, so, okay, so in the previous 5.
12:17:53 PM Marcely Zanon Boito: Yes. Yes.
12:17:58 PM André Martins: So before this one, there was a lot of data for Portuguese, but I tried to understand the deep.
12:18:03 PM Marcely Zanon Boito: Yes. So yes, this data for Portuguese is from minuting.
12:18:03 PM André Martins: Do? Okay, so what? Okay?
12:18:10 PM Marcely Zanon Boito: So that's why, when you go to ASR, we do not have a lot of data, because most of our Portuguese data is for minuting. However, it is not clear for us yet. If this data is transcribed or not.
12:18:25 PM Barry Haddow: Hmm.
12:18:26 PM Marcely Zanon Boito: I tried to download this data, but never cannot sign the contract for the loading this data. So we have to ask Edinburgh to do that in order to verify
12:18:35 PM André Martins: Okay.
12:18:35 PM Barry Haddow: So sorry. Sorry. What data set is this? The the large Portuguese
12:18:38 PM Marcely Zanon Boito: It's spotify the spotify is 76,000 h of minuting from podcasts. And I did request access, but then they send me a contract to sign, and then I asked people, never, and I cannot sign it.
12:18:47 PM Barry Haddow: Okay. Okay. Okay.
12:18:54 PM Vlad Niculae: I did I. I went through this a while ago, and I did sign the contract, and I think I did download at least the text I mean only the text, because the speech part is huge
12:18:54 PM Marcely Zanon Boito: So. Is it transcribed that is my question, because I know it's that they have minuting. But is it transcribed? Because if it's transcribed, then Portuguese is very well served for ASR
12:19:10 PM Vlad Niculae: It is transcribed. Yeah, but probably automatically, not manually.
12:19:11 PM Barry Haddow: Yeah, but yeah, I guess automatic. Yeah, so it's part of my corpus is mainly English and Portuguese.
12:19:11 PM Marcely Zanon Boito: Oh, so that's great! Hmm
12:19:19 PM Barry Haddow: Is that where you're saying the the spotify corpus is mainly English and Portuguese, or or hey it's only English and Portuguese.
12:19:21 PM Marcely Zanon Boito: Sir, can you repeat? It's only English and Portuguese. Yes.
12:19:29 PM Barry Haddow: Okay. Yeah.
12:19:31 PM Vlad Niculae: But I would also say, I'm not sure it's really mining, because it's podcasts and the summary is kind of like inside the spotify app every episode of a somebody provo, every episode of a podcast.
12:19:35 PM Barry Haddow: Hmm.
12:19:45 PM Vlad Niculae: Provides a summary. It's kind of like something plugged in by the
12:19:48 PM Marcely Zanon Boito: So so you would say it's more summarization than mining
12:19:52 PM Vlad Niculae: Well, I mean, what's the difference? Right? I mean, it's this, the summary, the spotify data set doesn't say, for instance, who said, what and what the conclusions are.
12:19:57 PM Barry Haddow: That's what we don't really know.
12:19:58 PM Marcely Zanon Boito: I mean, yeah. Hmm.
12:20:05 PM Barry Haddow: Yeah.
12:20:06 PM Vlad Niculae: It's more like in this episode. The hosts talk about geopolitical issues, or whatever you know.
12:20:10 PM Marcely Zanon Boito: I see. Hmm!
12:20:12 PM Barry Haddow: Yeah.
12:20:12 PM Vlad Niculae: So it's like it's a commercial summary
12:20:16 PM Barry Haddow: It's it's like a very brief summary of a dialogue, I suppose. What you're saying
12:20:21 PM Vlad Niculae: Yeah, to to the extent to to which you can consider a podcast a dialogue to cause, like it's.
12:20:22 PM Barry Haddow: Th. This meeting was about date Yeah, yeah.
12:20:29 PM Vlad Niculae: It's a very specific data set, but very useful because it's huge. Right?
12:20:31 PM Marcely Zanon Boito: Yes.
12:20:31 PM Vlad Niculae: So I've been thinking for a long time. That's very good to pre train stuff on it, probably, but it's also probably out of domain for anything we we care
12:20:40 PM Marcely Zanon Boito: Yeah, I mean, that depends on the kind of podcast tests that are included right? Because if you have those podcasting which there is conversation going on, then it is cases it's. It's pretty good. Otherwise if it's just a like news, then yeah, that that might be a bit less interesting
12:21:00 PM Barry Haddow: I mean, I mean, I think podcasts, are generally some kind of dialogue, but it's it's just a different kind of, perhaps a different kind of dialogue from what we think of as a meeting. But then meetings are is kind of a dive. Our set of, you know.
12:21:09 PM Marcely Zanon Boito: Hmm.
12:21:12 PM Barry Haddow: Is it a dive? Our set of things as well. So yeah, anyway.
12:21:15 PM Marcely Zanon Boito: Yes. Yeah. But well, then, if you go for emotional recommendation from speech, then we we might have a problem, because we do not find any data for the German or Portuguese. This, again, is much recognition from speech. We only have data for Korean, English, and French, and and this is because for French I found way more data sets. But the licenses are very difficult. So you have to sign stuff and send stuff. To places so we are not able to retrieve these data sets. So yes, and regarding much recognition speech. This was something that was identified as a task because we wanted to. Maybe we want to not wanted to. But the initial goal was ASR Reach ASR.
12:22:19 PM Marcely Zanon Boito: So we wanted to have also some sort of motion recognition, together with ASR. But I think so far we do not have enough data, emotional data annotated that could allow for that.
12:22:29 PM André Martins (chat): There is a speech group in Lisbon who have done some work on emotion detection on speech for Portuguese, I can ask around if there is some data There is a speech group in Lisbon who have done some work on emotion detection on speech for Portuguese, I can ask around if there is some data
12:22:34 PM Marcely Zanon Boito: So that is a possible issue identified at this point, denserization and minuting for whatever is the difference we have, then this spotify data set with a lot of data for Portuguese.
12:22:52 PM Marcely Zanon Boito: Since I didn't managed to deliver it. I still don't know which variety of Portuguese it is, but I'm guessing it's Brazilian. It might be European, I think the probabilities that it is. If African oration, it's a bit lower, and then we have 191 h of summarization in English as well. That we could use. And this is interesting is because one of these data that's from is earnings meetings. And this is accented data with summarization. So I found that that was nice, for already used case. So there was for a speech for text. We have over 3 billion synthesis collected 141 language directions from 53. This and data sets, we have a lot of data for machine translation. This is great. We also have some summaries for English, German, French, and Korean.
12:23:59 PM Marcely Zanon Boito: We also have emotion labels half a 1 million of synthesis with emotion labels in English, German, French, and Korean, and we also have dialogue data sets will be presented for English, German, Portuguese and and and frank so for machine translation here different
12:24:20 PM Marcely Zanon Boito: From the speech table. I we are considering English as private language, and this was something that was discussed with Jose. I think that for for for the use case for the customer care, use case. The the direction that we are. The the machine shows that we want to deliver is something that goes from English to other languages. We are not interested on translating, for instance, German into Dutch. So this is what we focused on. We see that we have for European languages. There is no problem. We have a lot of data for Korean. It's trickier. We have way, less data. So we have to to think about how to leverage us. Maybe some language models, text or something else in order to to make this happen regarding domains, we have legal literature.
12:25:22 PM Marcely Zanon Boito: Wikipedia Medical and 19 on the top 6. So pretty, diverse domains for image recognition from text. Again, it seems, for customer care. If I understood it correctly.
12:25:37 PM Marcely Zanon Boito: The go. It's it will be on to recognize emotions from text in English. So this is why most of the data that we have is in English here. And I, data collected in other languages. Here was many collected because we thought that this could be leverage for emotional recognition from speech, from for the the other case. So this is just kind of extra data that we could try to leverage if we really want to go for emotion, recognition from speech. And again we find no emotional data sets for Dutch and Portuguese. Finally we found a lot of text memorizations, data sets that we'll be also leveraged for minuting slides. Summarization, but none of them are on Dutch, and this is something that happen often is that Dutch is very poorly represented for all tasks.
12:26:40 PM Marcely Zanon Boito: So I was imagined that the Korean would be the one that the language that would suffer them with them, with the less quantity of data sets available. But that is also kind of ignored, apparently. So. This is a possible issue that we have to keep in mind. So summarizing for speech, recognition cool, considering only the data that we gathered, we find that ASR can be achieved for all the other languages that we have for Portuguese in Korean, we might have Chile or some Mono model, data but still there is and have speech in order.
12:27:20 PM Marcely Zanon Boito: To build something that is decent for speech translation. If we focus on English as source language, we can achieve trans translation, direct speech, translation for the other European languages. But we do not have enough Korean data for for this, and we do not have data for building systems from any to any language. For machine translation. We, though we do have enough data for much centralization in all other languages, a bit less on Korean.
12:27:55 PM Marcely Zanon Boito: But it is still enough data to build something decent. And thus, since we have machine transportation dating our languages, we can say that space translation is achievable because at least by applying speech translation cascaded speech translation is achievable for all other languages, so this means that for This 3 tasks. We are good to go. We are well covered for a much welcome.
12:28:18 PM André Martins: 1 one question so sorry. Can you go back? So here?
12:28:19 PM Marcely Zanon Boito: Yes, yes.
12:28:22 PM André Martins: Is some of these things are in need, directional, and others are bi-directional. Right? So for machine translation, I guess you can go in both directions, either from English or to English, or if we ignore transactionees and things like that.
12:28:30 PM Marcely Zanon Boito: Oh, yes, yes, that's
12:28:34 PM André Martins: But for speech, translation is different, right so, and you only go from English to those 4.
12:28:35 PM Marcely Zanon Boito: Yes, for speech. Pronunciation. Yes, yes, no.
12:28:39 PM André Martins: Okay, not the other way around I see.
12:28:43 PM Barry Haddow: But but that's only definitely for direct speech translation data.
12:28:47 PM André Martins: Yeah, yeah.
12:28:47 PM Barry Haddow: You're you're describing here. Yeah, yeah. So pipeline approach is phase feasible.
12:28:48 PM Marcely Zanon Boito: Yes. Yes, for all other languages. Yes, because since this speech recognition is achievable for all other languages, and much in translation is is doable as well.
12:28:52 PM Barry Haddow: For whenever, well, actually, for everything. Yeah, yeah.
12:28:55 PM André Martins: Yeah.
12:29:02 PM Marcely Zanon Boito: Then yes.
12:29:05 PM André Martins: Okay. Thanks.
12:29:06 PM Marcely Zanon Boito: So for image recognition. We have a general lack of emotion. Labels so joint ASR, and emotion recognition. Might be possible for French, English, or Korean, but we do not have a lot of data, and we still would have to check the quality of the labels for these languages. So here I would say no, for most recognition from text this can be achieved for English. And this extra data in other languages could be leveraged for the much recognition from speech. But it remains to be decided how we want to do it, or if we want to do it at all in the project for summarization.
12:29:48 PM André Martins: W. What one question about sorry, one question about the emotional recommendations, for from speech?
12:29:50 PM Marcely Zanon Boito: Yes. Hmm.
12:29:54 PM André Martins: Is there any work that tries to do? Is you know, to to combine multiple languages like legal emotional recognition? I guess for Korean it might be very different from the others, but for the European languages. Do you think it we could do leverage information that exactly from some languages to help with other languages? Or is this difficult?
12:30:13 PM Marcely Zanon Boito: I, I actually, I I really do not know. I'm not very familiar with much recognition from speech myself. But one thing that is challenging, I think, is that even between data sets, the labels are completely different. So there is not a very standard way of annotating emotional data. So, even if you want to mix 2 different data sets in French, that would already be difficult. So mixing from different languages, might increase the challenge, so I'm not sure.
12:30:44 PM Marcely Zanon Boito: But indeed, if we could build something, I'm like a multilingual motion recognition model that that would be great right
12:30:53 PM Chryssa Zerva: Is there a way to simplify the labels to something positive, mutual, or negative? I mean, I was trying to avoid that when I was looking at data sets, because that's more like sentiment justification, not in some recognition.
12:31:01 PM Marcely Zanon Boito: Hmm. Yes.
12:31:05 PM Chryssa Zerva: But maybe at this point we need to consider something like that. And in that case, at least, I know for text. We would also find more data
12:31:13 PM Marcely Zanon Boito: Yes, yes, that's that's yeah. That's a possibility. And that, I guess, would already be useful right if you are trying to build some sort of summarization system, if you know, at least at the sentence, is positive, neutral, negative. That's that's something already. Yes, that could definitely be a possibility.
12:31:35 PM Chryssa Zerva: And would be helpful also for the other use case or for the customer system. Great. It's too simplified
12:31:45 PM Marcely Zanon Boito: For the custom cares use case. I actually do not know. Does anybody want to answer this for customer care? If the simplifying, the emotion labels would still be useful for the feedback of the system
12:32:03 PM André Martins: Yeah, so we have a master student who is starting to work on that problem. And he's dealing with the problem that you know different data sets have different levels and is trying to simplify. And you know we still don't have the the answer to that question that we are exploring that
12:32:21 PM Marcely Zanon Boito: That's great. Okay, so
12:32:21 PM Evgenia Ilia (she/her) (chat): For emotion recognition: if we can code some correlation matrix for the labels, we can learn without ‘hard-coding a transformation’. For emotion recognition: if we can code some correlation matrix for the labels, we can learn without ‘hard-coding a transformation’.
12:32:23 PM André Martins: Also because in in customer care, you know some emotions are more important than others.
12:32:29 PM André Martins: For example, you know there is not so much thing about fear which is one of the 7 emotions that many papers that work on this place I consider well, and you know emotions like any, and so on. There, particularly relevant
12:32:44 PM Marcely Zanon Boito: Okay, so for summarization and miniature, we have a lot of data for English and Portuguese, and for in for text data, we have dating no languages, but Dutch. So from this, what I infer is that we could do summarization mining from speech in English and Portuguese, and we could enrich further the data that we have from speech with the data from text for English and Portuguese, so concluding we can transcribe and translate between
12:33:21 PM Marcely Zanon Boito: All languages, even though we do not have switched directory translation data. You know other languages. We can achieve this through a Sr. Does empty motion recognition for for the assistant meeting music was Use case, as it was discussed. It was the goal with should be some reach. ASR. So with available data is realistic for French, English, and Korean for customer care use case.
12:33:49 PM Marcely Zanon Boito: This is about feedback confidence of the model, and if the available data we can do it on English and for mining, we can do it for English and Portuguese. We cannot do it for other language, because we do not have any summarization or mining data with with speech. So these are the 2 languages that we could use. I find it's already pretty nice that it is not English, one only, because at first we were thinking that we would only be able to do this on English. So the possibility of maybe doing this importance as well.
12:34:24 PM Marcely Zanon Boito: I think this is nice in general, for all the tasks that Dutch is the least represented. Language, so we might want to keep an eye of it if we see data sets, can you release and stuff like that? If you know people that are working on that, if you can get some more data that we would be nice. And then finally, the the last slide is just to tell you, how can you use the output of this survey? They go with. The server was ready to save everybody's time looking for data. So the tables are interactive. You can open the tables. You can future test by language. By domain, you can find the data sets are there and easily download them.
12:35:12 PM Marcely Zanon Boito: The data. Statistics are already there. So if you look for speech, translation data sets, you already have the information of how much data is inside, how many hours, how how much of the data is annotated tokens types, and so on. And the tables are also editable. We made them on drive, and the goal of doing this is that this way, if you are working on your work package and you'll find a new data set that you want to use if you find that it's also useful for other people you can feel free to add them
12:35:44 PM Marcely Zanon Boito: At this collection of tables. This way we can can evolve the the or data set collection. So that's pretty much it. Thank you. Everybody
12:35:58 PM Barry Haddow: Thank you, Marcela, for putting all this together. Just let me tell you that you said the data set spreadsheet was editable, but the current
12:36:09 PM Marcely Zanon Boito: It should be but I I think I sent the wrong link right. I think I with the link you cannot edit. I'm going to update it on the
12:36:18 PM Barry Haddow: Okay, yeah. Cause the link I was adding is no longer editable. But I'm not sure. Yeah, that there comes a point where you're having it in a Google sheet isn't the ideal thing, because well, it has a lot of nice properties.
12:36:31 PM Marcely Zanon Boito: Yes, for sure.
12:36:34 PM Barry Haddow: But if if it's easy easy for me to go in and break the Google spreadsheet by accidental editing, so I'm not, I'm not sure what to do about version control, and so on.
12:36:41 PM Marcely Zanon Boito: Yes.
12:36:46 PM Barry Haddow: But
12:36:47 PM Marcely Zanon Boito: Well, I do have backups. So
12:36:50 PM Barry Haddow: Okay, okay, that's fine. Then I can break it if I want
12:36:52 PM Marcely Zanon Boito: Yes, I I saved Tsv. Versions of the table. So that's to see Tsv. Versions that I use for extracting the statistics and so on.
12:36:56 PM Barry Haddow: Yeah, yeah, yeah.
12:37:03 PM Marcely Zanon Boito: But yeah, so I'm gonna add on the chat the the link for the spreadsheets. Now it's LED full again. So yeah, I hope I hope it's it's useful for everybody
12:37:23 PM Evgenia Ilia (she/her): Yeah. Thanks everyone who contributed. I was wondering, do you want to? The first thing I just do want to publish this effort in a in a in a way that is also visible to outsiders
12:37:41 PM Marcely Zanon Boito: I don't know. Honestly, I would love to hear what the other people think about it. At first there was a talk going on about maybe doing a survey paper, or a wreck paper or something, but I don't know. I don't feel that there is anything really novel about the survey. It's just that we concentrated on what was interesting for us, and we gather a lot of data. But I don't know how the other partners feel about it.
12:38:10 PM Marcely Zanon Boito: The people that contribute, on these tables. What what would you like to do with it? Because in that we could maybe I don't know, have a blog post, and then some sort of non edible link for this information, or something
12:38:22 PM Evgenia Ilia (she/her): Yeah, I would. I use the word publishing a very general thing.
12:38:25 PM Marcely Zanon Boito: Hmm.
12:38:27 PM Evgenia Ilia (she/her): It could be something of that kind, some form of dissemination. Yeah.
12:38:33 PM Marcely Zanon Boito: Yeah, I guess I guess we could. We could make a blog post or something, and have maybe transferred the tables to on Tsv for a github or something, and to not share it directly to the drive, or
12:38:51 PM Chryssa Zerva: It will be nice to have a github with. Thank you. Links to the data, set something information. And if it's linked to block post, that would be when I said
12:39:03 PM Marcely Zanon Boito: Yeah, I think we think we could could do that. Probably
12:39:06 PM Vlad Niculae: If you wanted to be sightable you can get a fix share doi for the Github.
12:39:12 PM Vlad Niculae: The version.
12:39:14 PM Marcely Zanon Boito: I I didn't know that. That's that's cool
12:39:23 PM Evgenia Ilia (she/her): I have to ask you more about that And there, I think Andrea mentioned working on. Was it a motion, detection event? But then the question would be, are there on their projects already? And after that are exploiting some of these resources Hello! That we haven't yet right? Because we, when we started with big power data sets. No, without considering the survey
12:39:59 PM Marcely Zanon Boito: Neighbor is internally working on joint ASR and emotional recognition. So, and then the output of this. It's on a kind of broader project of uttering sign of neighbors. So the outputs of that project can be exploited in other. But yeah.
12:40:35 PM José Souza: Can you hear me?
12:40:37 PM Marcely Zanon Boito: Yeah. It was very, very low. Divide.
12:40:42 PM José Souza: Yeah, let me try to fix this
12:40:44 PM Vlad Niculae: Better not
12:40:46 PM Marcely Zanon Boito: Yeah. It's bad enough
12:40:47 PM José Souza: Hi, can you hear me? Okay, yeah, just about making this public. So for example, for machine translation, there is much more data. There are 3 billion there. But, for example, I didn't go for common crawl, and other crawled corpora. This could be even bigger and I don't know. Maybe if you want to make this public, we need to. So do a less sweet for maybe trying to find data that is missing, or some other interesting stuff But they end on apples. Yeah. Might be able to do
12:41:29 PM Marcely Zanon Boito: I mean that that depends on. If people are willing to spend their time doing that right? Because, yeah, I think that would be great to fill the table a bit more, because now we with the statistics we kind of can see what is missing. So we could imagine doing like a second smaller milestone in which we try to to field the gaps, and then we extract from the tables a sequence of links, and on a github something linked to a blog post, and then we put everything available
12:42:04 PM Marcely Zanon Boito: Think that's that would be nice. But then I do not know deadline for that
12:42:20 PM Evgenia Ilia (she/her): Yeah, there is no formal momentum connected directly connected with Ms. 2. So if we feel like doing that, we can give it more again. That attached to the roadmap as it's currently there on the on the weekend. I guess that just means that if it, if it happens that there are more efforts in this direction, 1 point, if you It's the right moment to put you on Github, or some of the form of public dissemination
12:42:53 PM Marcely Zanon Boito: Another another option would be to, to consider, to publish these resources, together with the next milestone for work, pack issue.
12:43:04 PM Marcely Zanon Boito: That is the first batch of data that we release because we have 3 milestones and work packages related to the data that we produce. So we could kind of put it together. Chip that with that. But then that would be waiting one year, because because the the first believerable with sign one year, I think so. That's that might be a lot of time. So yeah, so yeah, this is something.
12:43:31 PM Marcely Zanon Boito: You have you. We'll send off Doodle and talk with the people that work back issue and see what we but we can do about it
12:43:42 PM Barry Haddow: Hmm. So in terms of like data, creation, annotation, for example, is there anything we should focus on? Or is it not clear yet? I mean, we've got a lot of things we could work with
12:44:16 PM Marcely Zanon Boito: That depends. So it depends on the task. I would say, so, yeah, I think that the the best way of going forward would be now to inside the other work packages defining their next goals.
12:44:26 PM Barry Haddow: Yeah.
12:44:37 PM Marcely Zanon Boito: For instance, I think, work package. For now, once you have recognition and translation systems and then decided on the language directions and deciding cascaded verses. And to end and then based on all these specific needs, then decided if more data is needed, or not
12:45:02 PM Marcely Zanon Boito: Because this is something that this is something that is not clear from the statistics, and it's impossible to make it clear. That year I was talking with some Korean colleagues from Neighbor that developed speech translation for Papa Go, which is a a product from labor, and they have some. Sometimes you can have a lot of speech, translation data, and it's still not enough to make a general system.
12:45:28 PM Barry Haddow: Hmm, hmm, hmm.
12:45:28 PM Marcely Zanon Boito: So, so, yeah, this is, this might be something that is hidden as well. And once we try, we try and start making the systems, we see them fail. And then you might realize we need better data and more data, etc.
12:45:40 PM Barry Haddow: Hmm. So you're saying the next step is to build stuff with this data, which is, in fact, which, in fact, is the next project.
12:45:51 PM Marcely Zanon Boito: Yes.
12:45:53 PM Barry Haddow: Milson, I think, and then you know that we can see how good that stuff is. If it's not good enough, then the problem might be data, or it might be something else.
12:45:58 PM Marcely Zanon Boito: Hmm. Yes. Yes, and that's something that would be very nice from the other work packages to keep the information flowing. So if they are building stuff they can use or resources before looking at Google for data sets.
12:46:11 PM Barry Haddow: Yeah.
12:46:15 PM Marcely Zanon Boito: And then if they find that they do not have what they need, they should let us know this way. We we want this, know what is missing in terms of data, because we're back issue.
12:46:27 PM Marcely Zanon Boito: The goal is data and resources. So we are we are there for for for that
12:46:41 PM Evgenia Ilia (she/her): That's partly why I asked earlier if we know Projects within that there are exploiting some of this resources over and precisely. I guess there is this miles and 3 coming up where we're we had some sort of first release of software and then and and then model. Sorry
12:47:28 PM Evgenia Ilia (she/her): If there are, I mean we can. We can keep on thinking about how to to. If this decision on, if at 1 point there there, there are no other remarks or questions, I, suppose we leave the well, I actually, Barry did propose that we leave the Chat TV discussion for next time. Sounds, very very reasonable.
12:47:50 PM Vlad Niculae: Was it for next time, or for a separate meeting
12:47:50 PM Barry Haddow: I,
12:47:53 PM Evgenia Ilia (she/her): Actually
12:47:54 PM Barry Haddow: I think it's well we've got 10 min, I suppose. And I don't think 10 min work. I don't think we're gonna solve the chat problem at 10 min, anyway.
12:48:02 PM Vlad Niculae: Unless we ask Chad Gp. To solve it for us.
12:48:04 PM Barry Haddow: Yeah, okay.
12:48:04 PM Evgenia Ilia (she/her): Oh, I I tried. Didn't give me anything user
12:48:07 PM Barry Haddow: Have you if you tried like. Give it the details of our produce, and ask what we should do next. Did did you try that? Okay.
12:48:14 PM Evgenia Ilia (she/her): No.
12:48:18 PM Barry Haddow: So.
12:48:19 PM Maryam Hashemi Shabestari: May I ask, how urgent it is because either we can postpone it to the next research meeting, or we can plan an extra meeting before that state.
12:48:21 PM Marcely Zanon Boito (chat): https://docs.google.com/spreadsheets/d/1swmj2SrCU3U6SP5uaVouSUCZ0YsdUJsSomcYX3cfDsY/edit?usp=sharing
12:48:29 PM Maryam Hashemi Shabestari: So if it is urgent, and then we can plan something in between
12:48:32 PM Marcely Zanon Boito (chat): I'm sorry, but I need to leave at 12h50 for my train :)
12:48:32 PM Vlad Niculae (chat): Thanks a lot Marcely have a good trip
